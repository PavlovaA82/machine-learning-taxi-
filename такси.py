import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
df = pd.read_csv(r"D:\–¢–∞–∫—Å–∏\ncr_ride_bookings.csv")

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
print("–ö–æ–ª–æ–Ω–∫–∏:", list(df.columns))

# ---------- 2. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
df = df.dropna(how='all', axis=1)
df = df.dropna(subset=['Booking Value', 'Ride Distance'])

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
num_cols = ['Ride Distance', 'Avg VTAT', 'Avg CTAT']
for col in num_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

df = df.dropna(subset=['Ride Distance', 'Booking Value'])
df = df[df['Ride Distance'] > 0]

# ---------- 3. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ----------
cat_cols = ['Vehicle Type', 'Pickup Location', 'Drop Location', 'Booking Status']
for col in cat_cols:
    if col in df.columns:
        df[col] = df[col].astype(str)
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])

# ---------- 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ ----------
print("üß© –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã...")

# –ö–≤–∞–¥—Ä–∞—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è ‚Äî –æ—Ç—Ä–∞–∂–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–π —Ä–æ—Å—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏
df["distance_squared"] = df["Ride Distance"] ** 2

# –û—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –≤—Ä–µ–º–µ–Ω (–µ—Å–ª–∏ –µ—Å—Ç—å)
df["time_ratio"] = df["Avg VTAT"] / (df["Avg CTAT"] + 1e-5)

# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä: —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ √ó –≤—Ä–µ–º—è –ø–æ–¥–∞—á–∏
df["distance_vtat"] = df["Ride Distance"] * df["Avg VTAT"]

# –§–ª–∞–≥ –¥–∞–ª—å–Ω–∏—Ö –ø–æ–µ–∑–¥–æ–∫
df["is_long_trip"] = (df["Ride Distance"] > df["Ride Distance"].median()).astype(int)

# ---------- 5. –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–≥–æ–¥–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã ----------
print("üå¶Ô∏è –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è...")

np.random.seed(42)
# –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (¬∞C)
df["temperature"] = np.random.uniform(10, 40, len(df))
# –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –¥–æ–∂–¥—è (0 = —Å—É—Ö–æ, 1 = –¥–æ–∂–¥—å)
df["rain_intensity"] = np.random.choice([0, 1], len(df), p=[0.8, 0.2])
# –í–ª–∞–∂–Ω–æ—Å—Ç—å –≤–æ–∑–¥—É—Ö–∞ (%)
df["humidity"] = np.random.uniform(30, 90, len(df))

# –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç–æ—Ä: –¥–æ–∂–¥—å √ó —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
df["rain_effect"] = df["rain_intensity"] * df["Ride Distance"]

# ---------- 6. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ X –∏ y ----------
X = df[['Ride Distance', 'Avg VTAT', 'Avg CTAT', 'Vehicle Type',
        'Pickup Location', 'Drop Location', 'Booking Status',
        'distance_squared', 'time_ratio', 'distance_vtat',
        'is_long_trip', 'temperature', 'rain_intensity',
        'humidity', 'rain_effect']].fillna(0)

y = df['Booking Value']

print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {X.shape[1]} —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")

# ---------- 7. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ----------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ---------- 8. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ----------
models = {
    "Random Forest": RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, random_state=42)
}

results = {}

for name, model in models.items():
    print(f"‚öôÔ∏è –û–±—É—á–∞–µ–º {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    results[name] = {"MAE": mae, "RMSE": rmse, "R2": r2}

# ---------- 9. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ----------
comp = pd.DataFrame(results).T
print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
print(comp.round(4))

best_model_name = comp["R2"].idxmax()
print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model_name}")

best_model = models[best_model_name]

# ---------- 10. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ----------
if hasattr(best_model, "feature_importances_"):
    feat_imp = pd.DataFrame({
        "Feature": X.columns,
        "Importance": best_model.feature_importances_
    }).sort_values("Importance", ascending=False)

    print("\nüîç –¢–æ–ø-10 —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏:")
    print(feat_imp.head(10))

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–∏–π —Ä–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    plt.figure(figsize=(8, 5))
    plt.barh(feat_imp["Feature"].head(10), feat_imp["Importance"].head(10))
    plt.gca().invert_yaxis()
    plt.title("–í–∞–∂–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ Booking Value")

    # –í–º–µ—Å—Ç–æ show() —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
    plt.savefig("feature_importance.png")
    plt.close()
    print("üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ —Ñ–∞–π–ª 'feature_importance.png'")

# ---------- 11. –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∞ ----------
print("\n‚úÖ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–æ–≥–Ω–æ–∑—É...")

if len(X) > 0:
    sample = X.sample(1, random_state=42)
    pred = best_model.predict(sample)[0]
    print("\nüí∞ –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∞:")
    print(sample)
    print(f"\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Booking Value: {pred:.2f}")
else:
    print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞.")
